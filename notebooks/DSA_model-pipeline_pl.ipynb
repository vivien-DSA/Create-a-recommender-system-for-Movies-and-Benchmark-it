{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://surpriselib.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue; font-weight:bold\">Assignment todo list</h1>\n",
    "\n",
    "- Extract all the pipeline implemented function in a python script and import them in the notebook to use them.\n",
    "- Implement the *get_user_recommendation* function.\n",
    "- **Bonus** : Generalize the *get_trained_model* function to use any surprise model kwargs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-weight:bold\">TODO</p>\n",
    "\n",
    "- Extract the implemened functions in a python script and import them in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc94c16b640>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Dataset\n",
    "\n",
    "ratings = Dataset.load_builtin('ml-100k')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc94c092ac0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.dataset import DatasetAutoFolds\n",
    "\n",
    "def load_ratings_from_surprise() -> DatasetAutoFolds:\n",
    "    ratings = Dataset.load_builtin('ml-100k')\n",
    "    return ratings\n",
    "\n",
    "load_ratings_from_surprise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc94cdca7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from surprise import Reader\n",
    "\n",
    "ratings_filepath = Path('../data/movielens/ml-latest-small/ratings.csv')\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "ratings = Dataset.load_from_file(ratings_filepath, reader)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc94cdcab80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_ratings_from_file(ratings_filepath : Path) -> DatasetAutoFolds:\n",
    "    reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "    ratings = Dataset.load_from_file(ratings_filepath, reader)\n",
    "    return ratings\n",
    "    \n",
    "ratings_filepath = Path('../data/movielens/ml-latest-small/ratings.csv')\n",
    "load_ratings_from_file(ratings_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc94cd14d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(from_surprise : bool = True) -> DatasetAutoFolds:\n",
    "    data = load_ratings_from_surprise() if from_surprise else load_ratings_from_file()\n",
    "    return data\n",
    "\n",
    "data = get_data(from_surprise=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-weight:bold\">TODO</p>\n",
    "\n",
    "- Extract the implemened functions in a python script and import them in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.trainset.Trainset at 0x7fc94cd14dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1651)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.n_users, train.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-weight:bold\">TODO</p>\n",
    "\n",
    "- Change the *model_kwargs* argument in the *get_trained_model* function to make it usable for any surprise model (SVD, KNN, NMF, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "model = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fc94c16b5b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x7fc94c14fdc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.trainset import Trainset\n",
    "from  surprise.prediction_algorithms.algo_base import AlgoBase\n",
    "\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "\n",
    "def get_trained_model(model_class: AlgoBase, model_kwargs: dict, train_set: Trainset) -> AlgoBase:\n",
    "    model = model_class(sim_options = model_kwargs)\n",
    "    model.fit(train_set)\n",
    "    return model\n",
    "\n",
    "model_kwargs = {'sim_options': {'user_based': False, 'name': 'pearson'}}\n",
    "get_trained_model(KNNBasic, {'user_based': False, 'name': 'pearson'}, train)\n",
    "# {'sim_options': {'user_based': False, 'name': 'pearson'}} - **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='907', iid='143', r_ui=5.0, est=4.762202632237946, details={'was_impossible': False}),\n",
       " Prediction(uid='371', iid='210', r_ui=4.0, est=4.211045843957054, details={'was_impossible': False}),\n",
       " Prediction(uid='218', iid='42', r_ui=4.0, est=3.4576465941225325, details={'was_impossible': False}),\n",
       " Prediction(uid='829', iid='170', r_ui=4.0, est=4.07731408042207, details={'was_impossible': False}),\n",
       " Prediction(uid='733', iid='277', r_ui=1.0, est=3.0701961367499555, details={'was_impossible': False}),\n",
       " Prediction(uid='363', iid='1512', r_ui=1.0, est=3.601462078997732, details={'was_impossible': False}),\n",
       " Prediction(uid='193', iid='487', r_ui=5.0, est=3.7561068104413047, details={'was_impossible': False}),\n",
       " Prediction(uid='808', iid='313', r_ui=5.0, est=4.54216185300126, details={'was_impossible': False}),\n",
       " Prediction(uid='557', iid='682', r_ui=2.0, est=3.6357756491341084, details={'was_impossible': False}),\n",
       " Prediction(uid='774', iid='196', r_ui=3.0, est=2.376520149285778, details={'was_impossible': False})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.test(test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9378456428063894"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7395408044495279"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "def evaluate_model(model: AlgoBase, test_set: [(int, int, float)]) -> dict:\n",
    "    predictions = model.test(test_set)\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['RMSE'] = accuracy.rmse(predictions, verbose=False)\n",
    "    metrics_dict['MAE'] = accuracy.rmse(predictions, verbose=False)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 0.980150596704479, 'MAE': 0.980150596704479}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "def train_and_evalute_model_pipeline(model_class: AlgoBase, model_kwargs: dict = {},\n",
    "                                     from_surprise: bool = True,\n",
    "                                     test_size: float = 0.2) -> (AlgoBase, dict):\n",
    "    data = get_data(from_surprise)\n",
    "    train_set, test_set = train_test_split(data, test_size, random_state=42)\n",
    "    model = get_trained_model(model_class, model_kwargs, train_set)\n",
    "    metrics_dict = evaluate_model(model, test_set)\n",
    "    return model, metrics_dict\n",
    "\n",
    "my_model, metrics_dict = train_and_evalute_model_pipeline(KNNBasic)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sim_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-64f9b7a76888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evalute_model_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-7661be17dab6>\u001b[0m in \u001b[0;36mtrain_and_evalute_model_pipeline\u001b[0;34m(model_class, model_kwargs, from_surprise, test_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_surprise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmetrics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2b949b00c83f>\u001b[0m in \u001b[0;36mget_trained_model\u001b[0;34m(model_class, model_kwargs, train_set)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_trained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/recsys_students/lib/python3.8/site-packages/surprise/prediction_algorithms/matrix_factorization.pyx\u001b[0m in \u001b[0;36msurprise.prediction_algorithms.matrix_factorization.SVD.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sim_options'"
     ]
    }
   ],
   "source": [
    "my_model, metrics_dict = train_and_evalute_model_pipeline(SVD)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x7fc94c0925e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trained_model(KNNBasic, {'user_based': False, 'name': 'pearson'}, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-weight:bold\">TODO</p>\n",
    "\n",
    "- Add the other models (baseline, item based with cosine & pearson sim metrics, NMF, SVD)\n",
    "- Add the fit_time in the benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN user based cosine': {'RMSE': 1.0193536815834319,\n",
       "  'MAE': 1.0193536815834319},\n",
       " 'KNN user based pearson': {'RMSE': 1.0150350905205965,\n",
       "  'MAE': 1.0150350905205965},\n",
       " 'KNN item based cosine': {'RMSE': 1.0264295933767333,\n",
       "  'MAE': 1.0264295933767333},\n",
       " 'KNN item based pearson': {'RMSE': 1.041104054968961,\n",
       "  'MAE': 1.041104054968961}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "benchmark_dict = {}\n",
    "\n",
    "model_kwargs = {'user_based': True, 'name': 'cosine'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN user based cosine'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': True, 'name': 'pearson'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN user based pearson'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': False, 'name': 'cosine'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN item based cosine'] = metrics_dict\n",
    "\n",
    "model_kwargs = {'user_based': False, 'name': 'pearson'}\n",
    "knn, metrics_dict = train_and_evalute_model_pipeline(KNNBasic, model_kwargs)\n",
    "benchmark_dict['KNN item based pearson'] = metrics_dict\n",
    "\n",
    "\n",
    "benchmark_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNN user based with cosine similarity': {'RMSE': 1.0193536815834319,\n",
       "  'MAE': 1.0193536815834319},\n",
       " 'KNN user based with pearson similarity': {'RMSE': 1.0150350905205965,\n",
       "  'MAE': 1.0150350905205965}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_dict = {}\n",
    "\n",
    "model_dict_list = [\n",
    "    {\n",
    "        'model_name' : 'KNN user based with cosine similarity',\n",
    "        'model_class' : KNNBasic,\n",
    "        'model_kwargs' : {'user_based': True, 'name': 'cosine'}\n",
    "    },\n",
    "    {\n",
    "        'model_name' : 'KNN user based with pearson similarity',\n",
    "        'model_class' : KNNBasic,\n",
    "        'model_kwargs' : {'user_based': True, 'name': 'pearson'}\n",
    "    },\n",
    "]\n",
    "\n",
    "for model_dict in model_dict_list:\n",
    "    model, metrics_dict = train_and_evalute_model_pipeline(\n",
    "        model_dict['model_class'], model_dict['model_kwargs'])\n",
    "    benchmark_dict[model_dict['model_name']] = metrics_dict\n",
    "    model_dict['fitted_model'] = model\n",
    "    \n",
    "benchmark_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0187  1.0078  1.0135  1.0096  1.0084  1.0116  0.0041  \n",
      "MAE (testset)     0.8084  0.7987  0.8071  0.8025  0.7994  0.8032  0.0039  \n",
      "Fit time          1.27    1.05    1.12    1.15    1.26    1.17    0.09    \n",
      "Test time         3.60    3.31    2.77    3.26    2.91    3.17    0.30    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.01872866, 1.00775992, 1.01354856, 1.00955551, 1.00843408]),\n",
       " 'test_mae': array([0.80843632, 0.79869761, 0.8071084 , 0.80245938, 0.79940892]),\n",
       " 'fit_time': (1.272223949432373,\n",
       "  1.0536251068115234,\n",
       "  1.116072177886963,\n",
       "  1.1464581489562988,\n",
       "  1.2636258602142334),\n",
       " 'test_time': (3.5992281436920166,\n",
       "  3.3081600666046143,\n",
       "  2.7677388191223145,\n",
       "  3.261117935180664,\n",
       "  2.909980058670044)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-weight:bold\">TODO</p>\n",
    "\n",
    "- Create a function that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def get_user_recommendation(model: AlgoBase, user_id: int, k: int, data, movies : pandas.DataFrame\n",
    "                           ) -> pandas.DataFrame:\n",
    "    \"\"\"Makes movie recommendations a user.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model : AlgoBase\n",
    "            A trained surprise model\n",
    "        user_id : int\n",
    "            The user for whom the recommendation will be done.\n",
    "        k : int\n",
    "            The number of items to recommend.\n",
    "        data : FIXME\n",
    "            The data needed to do the recommendation.\n",
    "        movies : pandas.DataFrame\n",
    "            The dataframe containing the movies metadata (title, genre, etc)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Dataframe\n",
    "        A dataframe with the k movies that will be recommended the user. The dataframe should have the following\n",
    "        columns (movie_name : str, movie_genre : str, predicted_rating : float, true_rating : float)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - You should create other functions that are used in this one and not put all the code in the same function.\n",
    "        For example to create the final dataframe, instead of implemented all the code\n",
    "        in this function (get_user_recommendation), you can create a new one (create_recommendation_dataframe)\n",
    "        that will be called in this function.\n",
    "    - You can add other arguments to the function if you need to.\n",
    "    \"\"\"\n",
    "    # FIXME\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
